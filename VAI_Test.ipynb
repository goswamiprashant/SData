{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRt6mtg1qJf6"
      },
      "source": [
        "#Basic CNN on Screw Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGqDZrNUupyC"
      },
      "source": [
        "##Directory Structure Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rPwuY4prxEs",
        "outputId": "6a1201c2-fe6f-4105-80af-4ad3b3224e9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938130-1629953152/screw.tar.xz\n",
            "To: /content/screw.tar.xz\n",
            "100% 195M/195M [00:14<00:00, 13.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938130-1629953152/screw.tar.xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xKx8jYUsIRc"
      },
      "outputs": [],
      "source": [
        "!tar -xvf /content/screw.tar.xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gbL3WBpsQAf"
      },
      "outputs": [],
      "source": [
        "import os,shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbHwPUSZsnUd",
        "outputId": "7c245e21-aa43-4b4d-ab7c-fef71b2d442f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['good',\n",
              " 'manipulated_front',\n",
              " 'scratch_head',\n",
              " 'thread_top',\n",
              " 'scratch_neck',\n",
              " 'thread_side']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir('/content/screw/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4EJnY-lswSC"
      },
      "outputs": [],
      "source": [
        "os.mkdir('/content/screw/train/bad')\n",
        "pth = '/content/screw/test/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6ZTV3bVtLgC"
      },
      "outputs": [],
      "source": [
        "m=000\n",
        "for i in os.listdir('/content/screw/test'):\n",
        "  if i!='good':\n",
        "      for j in os.listdir(pth+i):\n",
        "          shutil.copy('/content/screw/test/'+i+'/'+j,'/content/screw/train/bad/'+str(m)+'.png')\n",
        "          m+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUUNHrtHt3-p"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkXJnFGJuxiV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig17wteiCLBc"
      },
      "outputs": [],
      "source": [
        "for i i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aCupa9hvDnj"
      },
      "outputs": [],
      "source": [
        "CHANNEL = 3 \n",
        "IMG_HEIGHT =128\n",
        "IMG_WIDTH =128 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe8Rbggs1ZGR"
      },
      "source": [
        "##Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIJ38MxJvO82"
      },
      "outputs": [],
      "source": [
        "def Model (IMG_HEIGHT,IMG_WIDTH,CHANNEL):\n",
        "   #Input Layer\n",
        "   input = tf.keras.layers.Input(shape=(IMG_HEIGHT,IMG_WIDTH,CHANNEL))\n",
        "   #1st Conv Layer\n",
        "   c1 = tf.keras.layers.Conv2D(filters=64,kernel_size = (3,3),activation='relu',kernel_initializer='he_normal',padding='same')(input)\n",
        "   c1 = tf.keras.layers.Dropout(.2)(c1)\n",
        "   c1=tf.keras.layers.MaxPooling2D(pool_size=(2,2))(c1)\n",
        "\n",
        "   #2nd Conv Layer\n",
        "   c2 = tf.keras.layers.Conv2D(filters=32,kernel_size = (3,3),activation='relu',kernel_initializer='he_normal',padding='same')(c1)\n",
        "   c2 = tf.keras.layers.Dropout(.3)(c2)\n",
        "   c2=tf.keras.layers.MaxPooling2D(pool_size=(2,2))(c2)\n",
        "\n",
        "   #3rd Conv Layer\n",
        "   c3 = tf.keras.layers.Conv2D(filters=16,kernel_size = (3,3),activation='relu',kernel_initializer='he_normal',padding='same')(c2)\n",
        "   c3 = tf.keras.layers.Dropout(.4)(c3)\n",
        "   c3=tf.keras.layers.MaxPooling2D(pool_size=(2,2))(c3)\n",
        "   \n",
        "   #4th Conv Layer\n",
        "   c4 = tf.keras.layers.Conv2D(filters=8,kernel_size = (3,3),activation='relu',kernel_initializer='he_normal',padding='same')(c3)\n",
        "   c4 = tf.keras.layers.Dropout(.5)(c4)\n",
        "   c4 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(c4)\n",
        "\n",
        "   #Flattening Conv Layer Output\n",
        "   f1 =tf.keras.layers.Flatten()(c4)\n",
        "\n",
        "   #1st dense layer\n",
        "   d1 = tf.keras.layers.Dense(units =16,activation='relu') (f1)\n",
        "   #2nd dense layer\n",
        "   d2 = tf.keras.layers.Dense(units=8 , activation='relu') (d1)\n",
        "   \n",
        "   #Output Layers\n",
        "   out = tf.keras.layers.Dense(units=2,activation ='softmax')(d2)\n",
        "   \n",
        "   model = tf.keras.Model(inputs=input,outputs=out)\n",
        "\n",
        "   return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6islGBiC0Hye"
      },
      "outputs": [],
      "source": [
        "model =Model(IMG_HEIGHT,IMG_WIDTH,CHANNEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4Nz_YkW1cqx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67RfLon32XsQ"
      },
      "outputs": [],
      "source": [
        "pth = \"/content/screw/train/\"\n",
        "pth1 = \"/content/screw/test1/\"\n",
        "for i in os.listdir(\"/content/screw/train\"):\n",
        "     l = int(len(pth+i)*.4)\n",
        "     for m,n in enumerate(os.listdir(pth+i)):\n",
        "           shutil.move(pth+i+\"/\"+n,pth1+i)\n",
        "           if m==l:\n",
        "                break\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjeLgIvg2Pql"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"/content/screw/test1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmxjVvBZ1yBh"
      },
      "outputs": [],
      "source": [
        "pnew =\"/content/screw/train\"\n",
        "pextended = \"/content/screw/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAoR00vt0xBt",
        "outputId": "8c316dd2-bb6b-412c-83ff-6d611d3cbd7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 432 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "#train data\n",
        "dir_path_tr =\"/content/screw/train/\"\n",
        "ImageFlowtrain = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.,rotation_range=10,width_shift_range=.1,height_shift_range=.1,zoom_range=.1,shear_range=10,validation_split=.25)\n",
        "##We are fitting the data to Image data generator.\n",
        "ImageGeneratortrain = ImageFlowtrain.flow_from_directory(dir_path_tr,target_size=(128,128),seed=10,batch_size=32,class_mode='categorical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-_nrYm89-Ht",
        "outputId": "ac95ce58-3f2d-4e39-e25f-894277e56e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 32 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "#train data\n",
        "dir_path_test =\"/content/screw/test1/\"\n",
        "ImageFlowtest = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.,rotation_range=10,width_shift_range=.1,height_shift_range=.1,zoom_range=.1,shear_range=10,validation_split=.25)\n",
        "##We are fitting the data to Image data generator.softmax\n",
        "ImageGeneratortest = ImageFlowtest.flow_from_directory(dir_path_test,target_size=(128,128),seed=10,batch_size=10,class_mode='categorical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnuidRrfqH-E"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOqTQKjt3OYR"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V7Z0V88_pnz"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',metrics='accuracy',loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "#model.fit_generator(ImageFlowtrain,class_weight=.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGkMA35TANw-",
        "outputId": "22e1ee05-c30f-4785-c69f-5b3e038d36c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 28s 2s/step - loss: 0.2773 - accuracy: 0.7014 - val_loss: 0.7043 - val_accuracy: 0.5312\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.2606 - accuracy: 0.7014 - val_loss: 0.6967 - val_accuracy: 0.5312\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.2619 - accuracy: 0.7014 - val_loss: 0.7005 - val_accuracy: 0.5312\n",
            "Epoch 4/10\n",
            " 7/14 [==============>...............] - ETA: 14s - loss: 0.2665 - accuracy: 0.6875"
          ]
        }
      ],
      "source": [
        "model.fit_generator(ImageGeneratortrain,epochs=10,validation_data=ImageGeneratortest,class_weight={0:.3,1:.7})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwGE3-R7BNf5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44_kkv2ZqRgs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1bd2WYiqU4l"
      },
      "source": [
        "#New Implemenatation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N-l3K6aqTjW",
        "outputId": "1ad4b235-f5d8-42fc-ed2c-a010e68a381d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'A-Hierarchical-Transformation-Discriminating-Generative-Model-for-Few-Shot-Anomaly-Detection'...\n",
            "remote: Enumerating objects: 267, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 267 (delta 6), reused 0 (delta 0), pack-reused 249\u001b[K\n",
            "Receiving objects: 100% (267/267), 2.08 MiB | 22.15 MiB/s, done.\n",
            "Resolving deltas: 100% (151/151), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/shellysheynin/A-Hierarchical-Transformation-Discriminating-Generative-Model-for-Few-Shot-Anomaly-Detection.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyzySaZwqc0K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/A-Hierarchical-Transformation-Discriminating-Generative-Model-for-Few-Shot-Anomaly-Detection')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWvQvtrerCnm"
      },
      "outputs": [],
      "source": [
        "!python -m pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqkSWkJ8rVGY",
        "outputId": "39214191-9564-4a78-b5c4-52ca8873f98d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mvtec_anomaly_detection.tar.xz: 0.00B [00:00, ?B/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 1566, in ftp_open\n",
            "    fw = self.connect_ftp(user, passwd, host, port, dirs, req.timeout)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 1588, in connect_ftp\n",
            "    persistent=False)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 2408, in __init__\n",
            "    self.init()\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 2418, in init\n",
            "    self.ftp.login(self.user, self.passwd)\n",
            "  File \"/usr/lib/python3.7/ftplib.py\", line 427, in login\n",
            "    resp = self.sendcmd('PASS ' + passwd)\n",
            "  File \"/usr/lib/python3.7/ftplib.py\", line 275, in sendcmd\n",
            "    return self.getresp()\n",
            "  File \"/usr/lib/python3.7/ftplib.py\", line 248, in getresp\n",
            "    raise error_perm(resp)\n",
            "ftplib.error_perm: 530 Login incorrect.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"main_train.py\", line 59, in <module>\n",
            "    opt.input_name = download_class_mvtec(opt)\n",
            "  File \"/content/A-Hierarchical-Transformation-Discriminating-Generative-Model-for-Few-Shot-Anomaly-Detection/Dataloaders/mvtec_loader.py\", line 152, in download_class_mvtec\n",
            "    trainset = MVTecDataset(class_name=opt.pos_class, is_train=True)  # images of size 224, 224\n",
            "  File \"/content/A-Hierarchical-Transformation-Discriminating-Generative-Model-for-Few-Shot-Anomaly-Detection/Dataloaders/mvtec_loader.py\", line 41, in __init__\n",
            "    self.download()\n",
            "  File \"/content/A-Hierarchical-Transformation-Discriminating-Generative-Model-for-Few-Shot-Anomaly-Detection/Dataloaders/mvtec_loader.py\", line 100, in download\n",
            "    download_url(URL, tar_file_path)\n",
            "  File \"/content/A-Hierarchical-Transformation-Discriminating-Generative-Model-for-Few-Shot-Anomaly-Detection/Dataloaders/mvtec_loader.py\", line 119, in download_url\n",
            "    urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 247, in urlretrieve\n",
            "    with contextlib.closing(urlopen(url, data)) as fp:\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 222, in urlopen\n",
            "    return opener.open(url, data, timeout)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 525, in open\n",
            "    response = self._open(req, data)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 543, in _open\n",
            "    '_open', req)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 503, in _call_chain\n",
            "    result = func(*args)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 1584, in ftp_open\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 1566, in ftp_open\n",
            "    fw = self.connect_ftp(user, passwd, host, port, dirs, req.timeout)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 1588, in connect_ftp\n",
            "    persistent=False)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 2408, in __init__\n",
            "    self.init()\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 2418, in init\n",
            "    self.ftp.login(self.user, self.passwd)\n",
            "  File \"/usr/lib/python3.7/ftplib.py\", line 427, in login\n",
            "    resp = self.sendcmd('PASS ' + passwd)\n",
            "  File \"/usr/lib/python3.7/ftplib.py\", line 275, in sendcmd\n",
            "    return self.getresp()\n",
            "  File \"/usr/lib/python3.7/ftplib.py\", line 248, in getresp\n",
            "    raise error_perm(resp)\n",
            "urllib.error.URLError: <urlopen error ftp error: error_perm('530 Login incorrect.')>\n"
          ]
        }
      ],
      "source": [
        "!python main_train.py  --num_images 10  --pos_class screw --dataset mvtec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI0tCJsor8vX",
        "outputId": "7a0b5bb9-0a37-4d14-acc9-b50eb850c78c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement kornia.augmentation.functional (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for kornia.augmentation.functional\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install kornia.augmentation.functional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axG-18kUrLdu"
      },
      "outputs": [],
      "source": [
        "--min_size                  image minimal size at the coarser scale (default 25)\n",
        "--max_size                  image minimal size at the coarser scale (default 64)\n",
        "--niter                     number of iterations to train per scale\n",
        "--num_images                number of images to train on (1,5,10 in the paper)\n",
        "--size_image                the original image size \n",
        "--fraction_defect           the number of patches to consider in defect detection (recommended arguments: 0.01-0.1)\n",
        "--pos_class                 the normal class to train on\n",
        "--dataset                   paris/cifar/mnist/fashionmnist/mvtec\n",
        "--random_images_download    \"True\" if training random images from the normal class (otherwise, specify the index of the training image in --index_download)\n",
        "--devices_ids                for 10shot we have used --device_ids = 0 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbvhq00OaNp9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGxTG706VKcU"
      },
      "outputs": [],
      "source": [
        "\n",
        "  \n",
        "# https://youtu.be/P9NdQG_vIvo\n",
        "\"\"\"\n",
        "Anomaly localization in images using the global average pooling layer.\n",
        "Binary classification - Good vs. bad images (Uninfected vs parasiized)\n",
        "This code uses the malarial data set but it can be easily applied to \n",
        "any application. \n",
        "Data from: https://lhncbc.nlm.nih.gov/LHC-publications/pubs/MalariaDatasets.html\n",
        "\"\"\"\n",
        "\n",
        "##########################################################\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.applications import vgg16\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import scipy  #Used to upsample our image\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6GnBKDjXSbz"
      },
      "outputs": [],
      "source": [
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNPamNfBWq7o"
      },
      "outputs": [],
      "source": [
        "for i,j in enumerate(l):\n",
        "    if i==500:\n",
        "        break\n",
        "    else:\n",
        "        shutil.copy('/content/cell_images/Uninfected/'+j,'/content/cellImages2/Uninfected/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO3EQrILWUs8"
      },
      "outputs": [],
      "source": [
        "l=os.listdir('/content/cell_images/Uninfected')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tebUqjkfVR4H"
      },
      "outputs": [],
      "source": [
        "image_directory = 'screw/test/'\n",
        "SIZE = 224\n",
        "dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
        "label = []  #Placeholders to define add labels. We will add 1 to all parasitized images and 0 to uninfected.\n",
        "\n",
        "parasitized_images = os.listdir(image_directory + 'good/')\n",
        "for i, image_name in enumerate(parasitized_images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
        "    \n",
        "    if (image_name.split('.')[1] == 'png'):\n",
        "        image = cv2.imread(image_directory + 'good/' + image_name)\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        dataset.append(np.array(image))\n",
        "        label.append(0)\n",
        "\n",
        "#Iterate through all images in Uninfected folder, resize to 224x224\n",
        "#Then save into the same numpy array 'dataset' but with label 0\n",
        "\n",
        "uninfected_images = os.listdir(image_directory + 'manipulated_front/')\n",
        "for i, image_name in enumerate(uninfected_images):\n",
        "    if (image_name.split('.')[1] == 'png'):\n",
        "        image = cv2.imread(image_directory + 'manipulated_front/' + image_name)\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        dataset.append(np.array(image))\n",
        "        label.append(1)\n",
        "\n",
        "\n",
        "\n",
        "dataset = np.array(dataset)\n",
        "label = np.array(label)\n",
        "\n",
        "#Split into train and test data sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.20, random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfM5t84SfrBh",
        "outputId": "b3e27267-3fc7-47db-df19-ba0066352df4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzFaDhZlVl7F",
        "outputId": "105ec432-2593-44fc-9217-57953a69394c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_3\n",
            "block1_conv1\n",
            "block1_conv2\n",
            "block1_pool\n",
            "block2_conv1\n",
            "block2_conv2\n",
            "block2_pool\n",
            "block3_conv1\n",
            "block3_conv2\n",
            "block3_conv3\n",
            "block3_pool\n",
            "block4_conv1\n",
            "block4_conv2\n",
            "block4_conv3\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " global_average_pooling2d_2   (None, 512)              0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,715,714\n",
            "Trainable params: 7,080,450\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.7449 - accuracy: 0.3846 - val_loss: 0.7480 - val_accuracy: 0.3077\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.7174 - accuracy: 0.4231 - val_loss: 0.6883 - val_accuracy: 0.5385\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.6809 - accuracy: 0.5962 - val_loss: 0.6478 - val_accuracy: 0.6923\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6687 - accuracy: 0.6154 - val_loss: 0.6288 - val_accuracy: 0.6923\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6623 - accuracy: 0.6154 - val_loss: 0.6224 - val_accuracy: 0.6923\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6627 - accuracy: 0.6154 - val_loss: 0.6204 - val_accuracy: 0.6923\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6647 - accuracy: 0.6154 - val_loss: 0.6202 - val_accuracy: 0.6923\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6643 - accuracy: 0.6154 - val_loss: 0.6224 - val_accuracy: 0.6923\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6593 - accuracy: 0.6154 - val_loss: 0.6281 - val_accuracy: 0.6923\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6567 - accuracy: 0.6154 - val_loss: 0.6337 - val_accuracy: 0.6923\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.6540 - accuracy: 0.6154 - val_loss: 0.6370 - val_accuracy: 0.6923\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6523 - accuracy: 0.6154 - val_loss: 0.6399 - val_accuracy: 0.6923\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6510 - accuracy: 0.6154 - val_loss: 0.6439 - val_accuracy: 0.6923\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6499 - accuracy: 0.6154 - val_loss: 0.6458 - val_accuracy: 0.6923\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6491 - accuracy: 0.6154 - val_loss: 0.6457 - val_accuracy: 0.6923\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6469 - accuracy: 0.6154 - val_loss: 0.6484 - val_accuracy: 0.6923\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6461 - accuracy: 0.6154 - val_loss: 0.6473 - val_accuracy: 0.6923\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6427 - accuracy: 0.6154 - val_loss: 0.6415 - val_accuracy: 0.6923\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6417 - accuracy: 0.6154 - val_loss: 0.6393 - val_accuracy: 0.6923\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6409 - accuracy: 0.6154 - val_loss: 0.6391 - val_accuracy: 0.6923\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6392 - accuracy: 0.6154 - val_loss: 0.6404 - val_accuracy: 0.6923\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6383 - accuracy: 0.6154 - val_loss: 0.6438 - val_accuracy: 0.6923\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6360 - accuracy: 0.6154 - val_loss: 0.6467 - val_accuracy: 0.6923\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6351 - accuracy: 0.6154 - val_loss: 0.6466 - val_accuracy: 0.6923\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6331 - accuracy: 0.6154 - val_loss: 0.6473 - val_accuracy: 0.6923\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6316 - accuracy: 0.6154 - val_loss: 0.6483 - val_accuracy: 0.6923\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6298 - accuracy: 0.6154 - val_loss: 0.6510 - val_accuracy: 0.6923\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6284 - accuracy: 0.6154 - val_loss: 0.6538 - val_accuracy: 0.6923\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6273 - accuracy: 0.6154 - val_loss: 0.6569 - val_accuracy: 0.6923\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.6271 - accuracy: 0.6154 - val_loss: 0.6526 - val_accuracy: 0.6923\n"
          ]
        }
      ],
      "source": [
        "#Without scaling (normalize) the training may not converge. \n",
        "#so that all values are within the range of 0 and 1.\n",
        "\n",
        "X_train = X_train /255.\n",
        "X_test = X_test /255.\n",
        "\n",
        "#Let us setup the model as multiclass with total classes as 2.\n",
        "#This way the model can be used for other multiclass examples. \n",
        "#Since we will be using categorical cross entropy loss, we need to convert our Y values to categorical. \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "\n",
        "#Define the model. \n",
        "#Here, we use pre-trained VGG16 layers and add GlobalAveragePooling and dense prediction layers.\n",
        "#You can define any model. \n",
        "#Also, here we set the first few convolutional blocks as non-trainable and only train the last block.\n",
        "#This is just to speed up the training. You can train all layers if you want. \n",
        "def get_model(input_shape = (224,224,3)):\n",
        "    \n",
        "    vgg = vgg16.VGG16(weights='imagenet', include_top=False, input_shape = input_shape)\n",
        "\n",
        "    #for layer in vgg.layers[:-8]:  #Set block4 and block5 to be trainable. \n",
        "    for layer in vgg.layers[:-5]:    #Set block5 trainable, all others as non-trainable\n",
        "        print(layer.name)\n",
        "        layer.trainable = False #All others as non-trainable.\n",
        "\n",
        "    x = vgg.output\n",
        "    x = GlobalAveragePooling2D()(x) #Use GlobalAveragePooling and NOT flatten. \n",
        "    x = Dense(2, activation=\"softmax\")(x)  #We are defining this as multiclass problem. \n",
        "\n",
        "    model = Model(vgg.input, x)\n",
        "    model.compile(loss = \"categorical_crossentropy\", \n",
        "                  optimizer = SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = get_model(input_shape = (224,224,3))\n",
        "print(model.summary())\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=16, epochs=30, verbose = 1, \n",
        "                    validation_data=(X_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XERSBRJDTwCJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "  \n",
        "# https://youtu.be/P9NdQG_vIvo\n",
        "\"\"\"\n",
        "Anomaly localization in images using the global average pooling layer.\n",
        "Binary classification - Good vs. bad images (Uninfected vs parasiized)\n",
        "This code uses the malarial data set but it can be easily applied to \n",
        "any application. \n",
        "Data from: https://lhncbc.nlm.nih.gov/LHC-publications/pubs/MalariaDatasets.html\n",
        "\"\"\"\n",
        "\n",
        "##########################################################\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.applications import vgg16\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import scipy  #Used to upsample our image\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "#Read images and get them ready for training\n",
        "\n",
        "image_directory = 'cell_images/'\n",
        "SIZE = 224\n",
        "dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
        "label = []  #Placeholders to define add labels. We will add 1 to all parasitized images and 0 to uninfected.\n",
        "\n",
        "parasitized_images = os.listdir(image_directory + 'Parasitized/')\n",
        "for i, image_name in enumerate(parasitized_images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
        "    \n",
        "    if (image_name.split('.')[1] == 'png'):\n",
        "        image = cv2.imread(image_directory + 'Parasitized/' + image_name)\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        dataset.append(np.array(image))\n",
        "        label.append(1)\n",
        "\n",
        "#Iterate through all images in Uninfected folder, resize to 224x224\n",
        "#Then save into the same numpy array 'dataset' but with label 0\n",
        "\n",
        "uninfected_images = os.listdir(image_directory + 'Uninfected/')\n",
        "for i, image_name in enumerate(uninfected_images):\n",
        "    if (image_name.split('.')[1] == 'png'):\n",
        "        image = cv2.imread(image_directory + 'Uninfected/' + image_name)\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        dataset.append(np.array(image))\n",
        "        label.append(0)\n",
        "\n",
        "dataset = np.array(dataset)\n",
        "label = np.array(label)\n",
        "\n",
        "#Split into train and test data sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.20, random_state = 0)\n",
        "\n",
        "#Without scaling (normalize) the training may not converge. \n",
        "#so that all values are within the range of 0 and 1.\n",
        "\n",
        "X_train = X_train /255.\n",
        "X_test = X_test /255.\n",
        "\n",
        "#Let us setup the model as multiclass with total classes as 2.\n",
        "#This way the model can be used for other multiclass examples. \n",
        "#Since we will be using categorical cross entropy loss, we need to convert our Y values to categorical. \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "\n",
        "#Define the model. \n",
        "#Here, we use pre-trained VGG16 layers and add GlobalAveragePooling and dense prediction layers.\n",
        "#You can define any model. \n",
        "#Also, here we set the first few convolutional blocks as non-trainable and only train the last block.\n",
        "#This is just to speed up the training. You can train all layers if you want. \n",
        "def get_model(input_shape = (224,224,3)):\n",
        "    \n",
        "    vgg = vgg16.VGG16(weights='imagenet', include_top=False, input_shape = input_shape)\n",
        "\n",
        "    #for layer in vgg.layers[:-8]:  #Set block4 and block5 to be trainable. \n",
        "    for layer in vgg.layers[:-5]:    #Set block5 trainable, all others as non-trainable\n",
        "        print(layer.name)\n",
        "        layer.trainable = False #All others as non-trainable.\n",
        "\n",
        "    x = vgg.output\n",
        "    x = GlobalAveragePooling2D()(x) #Use GlobalAveragePooling and NOT flatten. \n",
        "    x = Dense(2, activation=\"softmax\")(x)  #We are defining this as multiclass problem. \n",
        "\n",
        "    model = Model(vgg.input, x)\n",
        "    model.compile(loss = \"categorical_crossentropy\", \n",
        "                  optimizer = SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = get_model(input_shape = (224,224,3))\n",
        "print(model.summary())\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=16, epochs=30, verbose = 1, \n",
        "                    validation_data=(X_test,y_test))\n",
        "\n",
        "\n",
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "plt.plot(epochs, acc, 'y', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "###############################################################\n",
        "\n",
        "#Check model accuracy on the test data\n",
        "_, acc = model.evaluate(X_test, y_test)\n",
        "print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
        "\n",
        "#Test on single image.\n",
        "n=10  #Select the index of image to be loaded for testing\n",
        "img = X_test[n]\n",
        "plt.imshow(img)\n",
        "input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
        "print(\"The prediction for this image is: \", np.argmax(model.predict(input_img)))\n",
        "print(\"The actual label for this image is: \", np.argmax(y_test[n]))\n",
        "\n",
        "#Print confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "cm=confusion_matrix(np.argmax(y_test, axis=1), y_pred)  \n",
        "sns.heatmap(cm, annot=True)\n",
        "\n",
        "#############################################################\n",
        "#Save all images classified as parasited so we can fetch these images\n",
        "#later and plot heatmaps.\n",
        "########################################################\n",
        "#Identify all images classified as parasitized\n",
        "parasited_image_idx = np.where(y_pred == 1)[0]\n",
        "\n",
        "#Save all images classified as parasited to a directory (optional, makes sense for large data sets)\n",
        "#capture it in memory as an array\n",
        "predicted_as_para=[]\n",
        "for i in parasited_image_idx:\n",
        "    par_img = X_test[i]\n",
        "    #plt.imsave(\"results_classified_as_para/para_\"+str(i)+\".png\", par_img)\n",
        "    predicted_as_para.append(par_img)\n",
        "    \n",
        "predicted_as_para = np.array(predicted_as_para)    \n",
        "\n",
        "\n",
        "from matplotlib.patches import Rectangle #To add a rectangle overlay to the image\n",
        "from skimage.feature.peak import peak_local_max  #To detect hotspots in 2D images. \n",
        "def plot_heatmap(img):\n",
        "  \n",
        "    pred = model.predict(np.expand_dims(img, axis=0))\n",
        "    pred_class = np.argmax(pred)\n",
        "    #Get weights for all classes from the prediction layer\n",
        "    last_layer_weights = model.layers[-1].get_weights()[0] #Prediction layer\n",
        "    #Get weights for the predicted class.\n",
        "    last_layer_weights_for_pred = last_layer_weights[:, pred_class]\n",
        "    #Get output from the last conv. layer\n",
        "    last_conv_model = Model(model.input, model.get_layer(\"block5_conv3\").output)\n",
        "    last_conv_output = last_conv_model.predict(img[np.newaxis,:,:,:])\n",
        "    last_conv_output = np.squeeze(last_conv_output)\n",
        "    \n",
        "    #Upsample/resize the last conv. output to same size as original image\n",
        "    h = int(img.shape[0]/last_conv_output.shape[0])\n",
        "    w = int(img.shape[1]/last_conv_output.shape[1])\n",
        "    upsampled_last_conv_output = scipy.ndimage.zoom(last_conv_output, (h, w, 1), order=1)\n",
        "    \n",
        "    heat_map = np.dot(upsampled_last_conv_output.reshape((img.shape[0]*img.shape[1], 512)), \n",
        "                 last_layer_weights_for_pred).reshape(img.shape[0],img.shape[1])\n",
        "    \n",
        "    #Since we have a lot of dark pixels where the edges may be thought of as \n",
        "    #high anomaly, let us drop all heat map values in this region to 0.\n",
        "    #This is an optional step based on the image. \n",
        "    heat_map[img[:,:,0] == 0] = 0  #All dark pixels outside the object set to 0\n",
        "    \n",
        "    #Detect peaks (hot spots) in the heat map. We will set it to detect maximum 5 peaks.\n",
        "    #with rel threshold of 0.5 (compared to the max peak). \n",
        "    peak_coords = peak_local_max(heat_map, num_peaks=5, threshold_rel=0.5, min_distance=10) \n",
        "\n",
        "    plt.imshow(img.astype('float32').reshape(img.shape[0],img.shape[1],3))\n",
        "    plt.imshow(heat_map, cmap='jet', alpha=0.30)\n",
        "    for i in range(0,peak_coords.shape[0]):\n",
        "        print(i)\n",
        "        y = peak_coords[i,0]\n",
        "        x = peak_coords[i,1]\n",
        "        plt.gca().add_patch(Rectangle((x-25, y-25), 50,50,linewidth=1,edgecolor='r',facecolor='none'))\n",
        "\n",
        "    \n",
        "\n",
        "import random \n",
        "im = random.randint(0,predicted_as_para.shape[0]-1)\n",
        "heat_map =plot_heatmap(predicted_as_para[im])\n",
        "\n",
        "img = predicted_as_para[im]\n",
        "plt.imshow(predicted_as_para[im])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iRt6mtg1qJf6"
      ],
      "name": "VAI_Test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}